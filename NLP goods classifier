
OKP_list<-read.csv("C:/Users/msmirnov/Documents/проект УСН/Анализ данных/справочник ОКП (очищенный).csv", na.strings=c(" ", NA), stringsAsFactors=FALSE, header = TRUE, sep = ";")


OKP_list$id <- seq(length=nrow(OKP_list))
OKP_list<- as.data.frame(OKP_list [ , c(4,1,2,3)])



library(stringr)





# присваиваем признак "1" для позиции в подгруппе,т.е. названию начинающемуся с "-"
for (i in  1:nrow(OKP_list)) {
  
  OKP_list$sub [i]<-ifelse(str_locate(OKP_list [i,4], "-") == 2 , 1, 0) 
  
}

OKP_list$sub[is.na(OKP_list$sub)]<-""
OKP_list$sub<-ifelse(OKP_list$sub==0, "", OKP_list$sub)



for (i in  2:nrow(OKP_list)) {
    
    OKP_list$group [i]<-ifelse(OKP_list [i,5] == 1 && OKP_list [i-1,5] == "", OKP_list [i-1,4], "") 
    OKP_list$group [i]<-ifelse(OKP_list [i,5] == 1 && OKP_list [i-1,5] == 1, OKP_list$group [i-1], OKP_list$group [i]) 
        
      }

 
OKP_list$fulllName <- paste(OKP_list[ ,6], OKP_list[ ,4])

write.csv(OKP_list, 'C:/Users/msmirnov/Documents/проект УСН/Анализ данных/ОКП парсинг.csv')






  


OKP_list[grep("шампанск",OKP_list[,7]), ]
OKP_list[grep("картоф",OKP_list[,7]), ]



# Библиотеки R
# text2vec - для стемминга (отбрасывания окончаний)
# quanteda - для поиска косинусного расстояния между вектрами слов (например, всеми позициями в чеке и названиями в ОКП)

# Библиотеки Python
# pymorphy2 - для определения частей речи POS (возможно, понадобится для назаначения весовых коэффициентов)          
# gensim, word2vec - для тематического моделирования, т.е. поиска слов имеющих семантическую близость (вкл. синонимы)







# стемминг с помощью SnowballC (алгоритм Портера), который входит в библиотеку text2vec
library (text2vec)

# пишем функцию для стемминга
stem_tokenizer1 =function(x) {
  tokens = word_tokenizer(x)
  lapply(tokens, SnowballC::wordStem, language="ru")
}



# добавляем колонку в српавочник ОКП с названием после проведения стемминга
OKP_list$stem<-sapply(OKP_list[7],  stem_tokenizer1)


# в массив с содержанием 1-го конкретного чека добавляем колонку с названиями позиции после стемминга
checkStem<-as.data.frame(dat2[3,28])
checkStem$stem <-sapply(checkStem[4],  stem_tokenizer1)


FUN <- function (x) paste(unlist(x), collapse = " ")

checkStem$stemmedNames <-apply(checkStem[8], 1, FUN) 


# анализ текста  для поиска косинусного расстояния используется библиотека Quanteda
library(quanteda)

myCorpus <- corpus(c(check = paste(unlist(OKP_list[39145,8]), collapse = " "), # название группы товаров из ОКП 
                     
                     target1 = checkStem$stemmedNames )) # содержание наименований конкретного чека из общего массива чеков 


myDfmNoStop <- dfm(myCorpus, remove = stopwords("english"), stem = TRUE, remove_punct = TRUE)

sim <- textstat_simil(myDfmNoStop , 'check', method = "cosine", margin = "documents") # нашли косинусное расстояние между вектором со словами в чеке и в ОКП
# первые 2 позиции в чеке "сахар", поэтому у обоих косинусное рассотяние отлично от "0" при сравнении с позицией ОКП " Сахар - песок /  - из сахарной свеклы"



# внесение данных по косинусному расстоянию в чек 
checkStemFinal<-cbind(checkStem [,c(1:7)],  sim[c(2:length(sim))])

# получаем чек с указанием полей из ОКП и косинусного расстояния для каждой позиции
# далее можно будет выбрать с какой категорией коэф. больше и классифицирвать позицию из чека в эту категорию
colnames(checkStemFinal)[8] <- paste (OKP_list[39145,7], OKP_list[39145,2], "/", OKP_list[39145,3])















##############################################
# POS токенизация - определение частей речи (возможно пригодиться для выставления весовых коэффициентов, т.к. семантически совпадения по существительным 
# дает бОльшую смысловую нагрузку, чем по прилагательным)

library(reticulate) # для работы с Python


# дистрибутивная семантика
# запускаем python скрипт, который загружает натренированную модель в память
setwd("C:/Users/msmirnov/")
source_python("C:/Users/msmirnov/Documents/R scripts/gensim_distr_semantics.py")

# загрузка модели без использования python скрипта
model<-gensim$models$KeyedVectors$load_word2vec_format("C:/Users/msmirnov/model.bin", binary=TRUE)

model$wv$most_similar("газовый_ADJ") # выводим перечень слов из модели, которые семантически близки к заданному

model$wv$most_similar(t)

model$wv$most_similar("смесь_NOUN")


t<- paste("смесь", "_NOUN", sep="")












lib <- import("pymorphy2")


phrase = r_to_py(phrase) #передача объекта в python



morph = lib$MorphAnalyzer()


library(stringr)



# POS токенизация после стемминга
for (i in 1:length(unlist(checkStem[3,8]))) { 
  
  
  
  print(str_sub(morph$tag(unlist(checkStem[3,8])[i])[[1]], start=1, end=4))
  
  
}




#word_tokenizer(checkStem[11,4]) # токенизация из библиотеки text2vec


# токенизация до стемминга (учитывает окончания и правильно определяет часть речи)
for (i in 1:length(unlist(checkStem[3,8]))) { 
  
  
  
  print(str_sub(morph$tag(unlist(word_tokenizer(checkStem[3,4]))[i])[[1]], start=1, end=4))
  
  
}
